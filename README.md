# FoxNN

[Установка](https://github.com/RadioRedFox/FoxNN/wiki/%D0%A3%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B0)

# Мануал

## Тестовые данные

[Работа с обучающей выборкой](https://github.com/RadioRedFox/FoxNN/wiki/%D0%A0%D0%B0%D0%B1%D0%BE%D1%82%D0%B0-%D1%81-%D0%BE%D0%B1%D1%83%D1%87%D0%B0%D1%8E%D1%89%D0%B5%D0%B9-%D0%B2%D1%8B%D0%B1%D0%BE%D1%80%D0%BA%D0%BE%D0%B9.)

## Создание нейронной сети

Допустим нам надо создать нейронную сеть, получающую на вход 2 параметра, имеющую один скрытый слой с 4 нейронами, 3 нейрона на последнем слое и на выход сеть выдавала бы 2 числа. 

Сеть задаётся вектором, в котором первые n чисел указывают на число нейронов в n-слоях, и n+1-ое число указывает сколько чисел на выход.   
В нашем случае получаем следующее:  
На С++:
```cpp
#include "foxnn.h"
vector <int> nn_parameters = {2, 4, 3, 2};
neural_network nn(nn_parameters);
```
На Python
```
import foxnn
nn = foxnn.neural_network([2, 4, 3, 2])
```

По умолчанию все слои имеют функцию активации сигмоиду. Если хотите поменять на другую, то в коде на С++ можно переопределить свою или воспользоваться уже описанными: sigmoid, sinusoid, gaussian, relu  
Установим на скрытый слой gaussian.  
C++
```cpp
nn[1] = "gaussian"
```
Python:
```python
nn.get_layer(1).set_activation_function("gaussian") #swig не переопределяет [] и =
```
Можно сделать надстроки на выходной слой.
1. Поиск максимума в выходном слое.  
Пусть у нас на выход после relu сеть выдаёт три числа [1, 4, 2].  
Если установить nn.settings.max_on_last_layer = 1 (для Python = True), то на выход мы получим [0, 1, 0]

2. Если выходное значение больше определённого значения, то 1, если нет то 0.
Пусть у нас на выдод после sigmoid сеть выдаёт три числа [0.1, 0.5, 0.9].
Если установить nn.settings.one_if_value_greater_intermediate_value = 1 (для Python = True) и 
nn.settings.intermediate_value = 0.5 (=0.5 по умолчанию) то на выход мы получим [0, 1, 1]

## Обучение

```python
nn.train_on_file("data.txt", 0.001, 10000, 0.0001, 100, 400)
#или
nn.train_on_data(data, 0.001, 10000, 0.0001, 100, 400)
```
Далее - номер передаваемого параметра в train_on_file - затем его объяснение.  
1 - файл в котором хранятся тестовые данные, либо как во втором случае уже считанные тестовые данные.  
2 - скорость обучения.  
3 - колличество итераций.  
4 - Размер средней ошибки до которой будет идти обучение. Подсчёт правильных ответов нейронки использует этот же параметр. Ответ правильный если abs(ответ_нейронки - требуемое значение) < этот параметр  
5 - Число итераций между выводом информации на экран.  
6 - Размер батча. По умолчанию = 1.

## OpenMP

Если размер батча достаточно большой и мощности вашего компьютера позволяют, то можно ускорить процесс обучения расспараллелив процесс.  
nn.settings.n_threads = 8 где 8 это число процессов. Если установить 0, то распараллеливание будет на максимально доступное число процессоров. 

## Мутация

### Обычная мутация 

nn.random_mutation(speed) - смещение значения всех весов на рандомное число от (-1, 1) * speed

### Умная мутация

nn.smart_mutation(speed) - Лучше на примере

nn.smart_mutation(0.1), а значение весов 100. Тогда новое значение будет взято из отрезка  
(100 - 100 * 0.1; 100 + 100 * 0.1) т.е. (90, 110)  
nn.smart_mutation(0.01), а значение весов 10000, тогда новое значение будет взято из отрезка (9900;10100).
